

# ニューラルネットワークの構造と学習のまとめ

## 理論編

ニューラルネットワークは入力層、中間層、出力層の3つで構成されている。
入力層は入力された内容を中間層に引き渡す。
中間層は重みの計算をする。
出力層は計算結果を出力している。

ニューラルネットワークの学習は、順伝播、誤差計算、逆伝播の3つで構成されている。
先程の入力層、中間層、出力層の3つの層を、順伝播、誤差計算、逆伝播でそれぞれ1周して計算される。

以下、イメージ

- 順伝播工程
    1. 入力層
    2. 中間層
    3. 出力層
- 誤差計算工程
    1. 順伝播の出力層の予測値と正解との誤差を損失関数で求める
- 逆伝播工程
    1. 出力層
    2. 中間層
    3. 入力層

順伝播では、重みの計算を行った上で予測を行う。
重み計算で使用された四則演算、行列積、活性化関数(ReLU,Sigmoid)などが多層的に(入れ子構造的に)なったものを「合成関数」と呼ぶ
予測結果を、後続の誤差計算の工程に引き渡す。

誤差計算では、順伝播から受け取った予測をもとに、正解との誤差を計算する。
この誤差計算を行う関数を、損失関数という。誤差とは正解と予測の数値的乖離であり、具体的な修正方法までは確認できない。誤差と損失はほぼ同義である。
代表的な損失関数には平均二乗誤差(MSE)、平均絶対誤差(MAE)、クロスエントロピーなどがある。
ちなみに、過学習を防ぐためにここで正則化の計算が使われることがある。L1正則化で効果のない学習の重みを0にし、L2正則化で特定パターンに偏らないように重みを全体的に小さくする。
この工程で計算した誤差を、後続の逆伝播の工程に引き渡す。

逆伝播では、誤差計算で受け取った誤差を使って、重みの更新を行う。
まずは誤差を、誤差逆伝播法(連鎖律)を使って勾配を導き出す。誤差逆伝播法(連鎖律)は、順伝播で使った「合成関数」の微分で導き出すことができる。
勾配とは、重みを具体的にどのように更新したら良いかがわかる数値のことである。
この勾配をもとに勾配降下法を使って、重みの更新ができる。
この勾配降下法を使った重みの更新時には、正則化の計算結果も反映され、過学習を抑止できる。

この 順伝播 → 誤差計算 → 逆伝播 を繰り返すことで学習を重ねることができる。
指定されたエポック数(学習回数)分だけ、順伝播 → 誤差計算 → 逆伝播 を繰り返すが、ここで何度も繰り返し学習を行っていると、モデルが複雑になり過学習状態になってしまう可能性がある点に注意する。
過学習は、適切な学習回数の設定、高品質な学習データの用意、テストと学習データの分離、正則化を適切に行うことで抑止できる。

## 実装編



